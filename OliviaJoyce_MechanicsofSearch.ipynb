{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8802772e",
   "metadata": {},
   "source": [
    "# PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "db600e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "\n",
    "import math\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import operator\n",
    "#from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b2241",
   "metadata": {},
   "source": [
    "# DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e24b7a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_queries(txt):\n",
    "    parsed_queries = []\n",
    "    queries = re.findall(r'<top>(.*?)</top>', txt, re.DOTALL)\n",
    "    counter = 1  # Initialize counter\n",
    "    for q in queries:\n",
    "        query = {}\n",
    "      #  query['num']  = re.search(r'<num>(.*?)</num>', q).group(1).strip() if re.search(r'<num>(.*?)</num>', q) else None\n",
    "        query['num'] = counter\n",
    "        query['title'] = re.search(r'<title>(.*?)</title>', q, re.DOTALL).group(1).strip() if re.search(r'<title>(.*?)</title>', q, re.DOTALL) else None\n",
    "        parsed_queries.append(query)\n",
    "        counter += 1\n",
    "    return parsed_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0eb388cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(relative_path):\n",
    "    all_data = []\n",
    "    full_path = os.path.join(os.getcwd(), relative_path)\n",
    "    for filename in os.listdir(full_path):\n",
    "        with open(os.path.join(full_path, filename), 'r', encoding='utf-8') as file:\n",
    "            data = parse_queries(file.read())\n",
    "            all_data.extend(data)  # Assuming you want to collect data from all files\n",
    "    return all_data\n",
    "\n",
    "queries = read_queries('queries')\n",
    "#print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84aa1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docs(txt):\n",
    "    docs = re.findall(r'<doc>(.*?)</doc>', txt, re.DOTALL)\n",
    "    parsed_data = []\n",
    "    for doc in docs:\n",
    "        doc_data = {}\n",
    "        doc_data['docno'] = re.search(r'<docno>(.*?)</docno>', doc).group(1).strip() if re.search(r'<docno>(.*?)</docno>', doc) else None\n",
    "        doc_data['title'] = re.search(r'<title>(.*?)</title>', doc, re.DOTALL).group(1).strip() if re.search(r'<title>(.*?)</title>', doc, re.DOTALL) else None\n",
    "        doc_data['author'] = re.search(r'<author>(.*?)</author>', doc).group(1).strip() if re.search(r'<author>(.*?)</author>', doc) else None\n",
    "        #doc_data['bib'] = re.search(r'<bib>(.*?)</bib>', doc).group(1).strip() if re.search(r'<bib>(.*?)</bib>', doc) else None\n",
    "        doc_data['text'] = re.search(r'<text>(.*?)</text>', doc, re.DOTALL).group(1).strip() if re.search(r'<text>(.*?)</text>', doc, re.DOTALL) else None\n",
    "        parsed_data.append(doc_data)\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ed781627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(relative_path):\n",
    "    all_data = []\n",
    "    full_path = os.path.join(os.getcwd(), relative_path)\n",
    "    for filename in os.listdir(full_path):\n",
    "        with open(os.path.join(full_path, filename), 'r', encoding='utf-8') as file:\n",
    "            data = parse_docs(file.read())\n",
    "            all_data.extend(data)  # Assuming you want to collect data from all files\n",
    "    return all_data\n",
    "\n",
    "docs = read_docs('docs')\n",
    "#print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5450fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "def preprocess(text):\n",
    "    # Tokenization and Lowercasing\n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    # Remove punctuation, stopwords, and perform stemming\n",
    "    processed_tokens = []\n",
    "    for token in tokens:\n",
    "        # Remove punctuation and check if token is not empty after stripping\n",
    "        token = token.strip(string.punctuation)\n",
    "        if token != '' and len(token) >= 2:\n",
    "            # Perform stemming and filter out stopwords\n",
    "            stemmed_token = porter.stem(token)\n",
    "            if stemmed_token not in stop_words:\n",
    "                processed_tokens.append(stemmed_token)  \n",
    "    return processed_tokens  #return as string   #' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6c5b3a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "###Check###\n",
    "num_docs = len(docs)\n",
    "print(num_docs)\n",
    "num_queries = len(queries)\n",
    "print(num_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aafdcbe",
   "metadata": {},
   "source": [
    "# INVERTED INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1e21f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using list of dictionaries data structure\n",
    "def create_inverted_index(data):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc in data:\n",
    "        doc_id = doc['docno']\n",
    "        text = doc['text']\n",
    "        terms = preprocess(text)\n",
    "        term_positions = defaultdict(list)  \n",
    "        # Populate term positions\n",
    "        for position, term in enumerate(terms, start=1):\n",
    "            term_positions[term].append(position)      \n",
    "        # Populate inverted index\n",
    "        for term, positions in term_positions.items():\n",
    "            term_frequency = len(positions)  # Calculate term frequency\n",
    "            posting = {\"doc\": doc_id, \"term_frequency\": term_frequency, \"positions\": positions}\n",
    "            inverted_index[term].append(posting)\n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = create_inverted_index(docs) # Create inverted index\n",
    "# Save inverted index to a file to view\n",
    "with open(\"inverted_index.txt\", \"w\") as file:\n",
    "    for term, postings in inverted_index.items():\n",
    "        file.write(f\"{term}: {postings}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27404a",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8583fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(inverted_index, docs):\n",
    "    tfidf = {}\n",
    "    N = len(docs)  # Total number of documents\n",
    "    # Compute document frequencies (df) for each term\n",
    "    df = {term: len(postings) for term, postings in inverted_index.items()}   \n",
    "    # Compute TF-IDF for each term-document pair\n",
    "    for term, postings in inverted_index.items():\n",
    "        idf = math.log(N / (df[term] + 1))  # Add 1 to avoid division by zero\n",
    "        for posting in postings:\n",
    "            doc_id = posting['doc']\n",
    "            tf = posting['term_frequency']\n",
    "            tfidf_score = tf * idf\n",
    "            tfidf.setdefault(doc_id, {})[term] = tfidf_score \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef038f3f",
   "metadata": {},
   "source": [
    "# Vector Space Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7dd1026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Docs TF-IDF scores\n",
    "docs_tfidf = compute_tfidf(inverted_index, docs)\n",
    "# Compute document frequencies (df) for each term  (needed for query TF-IDF computation)\n",
    "df = {term: len(postings) for term, postings in inverted_index.items()}\n",
    "# Compute total number of documents (N)\n",
    "N = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c6d0ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_tfidf(query_terms, docs_tfidf, df, N):\n",
    "    query_tfidf = {}\n",
    "    for term in query_terms:\n",
    "        # Calculate IDF for term\n",
    "        idf = math.log(N / (df.get(term, 0) + 1))\n",
    "        tfidf = 1 * idf\n",
    "        query_tfidf[term] = tfidf\n",
    "    return query_tfidf\n",
    "\n",
    "# Calculate cosine similarity (vec1 and vec2 are dictionaries)\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])    \n",
    "    sum1 = sum([val ** 2 for val in vec1.values()])\n",
    "    sum2 = sum([val ** 2 for val in vec2.values()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)    \n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "    \n",
    "def rank_vsm(query, docs_tfidf, df, N):\n",
    "    preprocessed_query = preprocess(query['title'])\n",
    "    query_tfidf = compute_query_tfidf(preprocessed_query, docs_tfidf, df, N)    \n",
    "    scores = []\n",
    "    for doc_id, doc_tfidf in docs_tfidf.items():\n",
    "        score = cosine_similarity(query_tfidf, doc_tfidf)\n",
    "        scores.append((doc_id, score))   \n",
    "    ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs\n",
    "\n",
    "def evaluate(queries, docs_tfidf, df, N):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        ranked_docs = rank_vsm(query, docs_tfidf, df, N)\n",
    "        for doc_id, score in ranked_docs:\n",
    "            results.append((query['num'], doc_id, score))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028cea4",
   "metadata": {},
   "source": [
    "# Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "001acc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(results, file_path, run_name):\n",
    "    with open(file_path, 'w') as f:\n",
    "        rank = 1  # Initialize rank counter\n",
    "        for result in results:\n",
    "            if result[2] != 0.0:  # Exclude results with score 0.0\n",
    "                query_id =  str(result[0])\n",
    "                document_id =  str(result[1])\n",
    "                score = result[2]\n",
    "                # Write in TREC format: <query_id> <Q0> <doc_id> <rank> <score> <run_id>\n",
    "                f.write(f\"{query_id} Q0 {document_id} {rank} {score} {run_name}\\n\")\n",
    "                rank += 1  # Increment rank for the next document\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6211ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and Output VSM\n",
    "vsm_results = evaluate(queries, docs_tfidf, df, N)\n",
    "write_results(vsm_results, 'vsm_results.txt', 'VSM_Run')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628bf72",
   "metadata": {},
   "source": [
    "# BM25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68741bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing idf - N is total numbe of documents, doc_freq is document frequency of term\n",
    "def idf(term, N, doc_freq):\n",
    "    return math.log((N - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c5aadd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bm25(inverted_index, docs, k1=1.5, b=0.75):\n",
    "    bm25_scores = {}\n",
    "    N = len(docs)\n",
    "    avgdl = sum(len(doc['text'].split()) for doc in docs) / N  # Average document length\n",
    "    df = {term: len(postings) for term, postings in inverted_index.items()}  # Document frequency\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_id = doc['docno']\n",
    "        doc_len = len(doc['text'].split())\n",
    "        bm25_scores[doc_id] = {}\n",
    "        \n",
    "        for term, postings in inverted_index.items():\n",
    "            idf_val = idf(term, N, df[term])\n",
    "            term_freq = sum(posting['term_frequency'] for posting in postings if posting['doc'] == doc_id)\n",
    "            # Calculate BM25 term score\n",
    "            term_score = idf_val * (term_freq * (k1 + 1)) / (term_freq + k1 * (1 - b + b * (doc_len / avgdl)))\n",
    "            if term_score > 0:\n",
    "                bm25_scores[doc_id][term] = term_score\n",
    "    return bm25_scores\n",
    "\n",
    "bm25_scores = compute_bm25(inverted_index, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "feba7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_bm25(query, bm25_scores):\n",
    "    query_terms = preprocess(query['title'])  # Assuming the query is in 'text' key\n",
    "    doc_scores = {}\n",
    "\n",
    "    for doc_id, scores in bm25_scores.items():\n",
    "        doc_score = sum(scores.get(term, 0) for term in query_terms)\n",
    "        if doc_score > 0:\n",
    "            doc_scores[doc_id] = doc_score\n",
    "\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5592e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bm25(bm25_scores, queries, docs):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        ranked_docs = rank_bm25(query, bm25_scores)\n",
    "        for doc_id, score in ranked_docs:\n",
    "            # Note: Ensure your query objects have a 'num' field that uniquely identifies them\n",
    "            results.append((query['num'], doc_id, score))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "48561502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and Output BM25\n",
    "evaluation_results = evaluate_bm25(bm25_scores, queries, docs)\n",
    "write_results(evaluation_results, 'bm25_results.txt', 'BM25_Run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a3d5f",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c81282b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Query likelihood with Jelinek-Mercer smoothing ##\n",
    "def compute_query_likelihood(query, inverted_index, total_terms, smoothing_param=0.1):\n",
    "    query_terms = preprocess(query['title'])\n",
    "    doc_likelihoods = {}\n",
    "    collection_frequencies = {term: sum(posting['term_frequency'] for posting in postings) for term, postings in inverted_index.items()}\n",
    "    total_collection_terms = sum(collection_frequencies.values())\n",
    "    \n",
    "    for term in query_terms: \n",
    "        if term in inverted_index:\n",
    "            postings = inverted_index[term]\n",
    "            for posting in postings:\n",
    "                doc_id = posting['doc']\n",
    "                term_freq = posting['term_frequency']\n",
    "                doc_length = total_terms.get(doc_id, 0)\n",
    "                term_prob = (1 - smoothing_param) * (term_freq / doc_length) + smoothing_param * (collection_frequencies.get(term, 0) / total_collection_terms)\n",
    "                doc_likelihoods[doc_id] = doc_likelihoods.get(doc_id, 1) * term_prob\n",
    "    \n",
    "    return doc_likelihoods\n",
    "\n",
    "#  rank documents based on query likelihood\n",
    "def rank_qlm(query_likelihoods):\n",
    "    return sorted(query_likelihoods.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# evaluate Query Likelihood Model\n",
    "def evaluate_qlm(queries, inverted_index, total_terms, smoothing_param=0.1):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        query_likelihoods = compute_query_likelihood(query, inverted_index, total_terms, smoothing_param)\n",
    "        ranked_docs = rank_qlm(query_likelihoods)\n",
    "        for doc_id, likelihood in ranked_docs:\n",
    "            results.append((query['num'], doc_id, likelihood))\n",
    "    return results\n",
    "\n",
    "# Compute total number of terms per document (The Document Model)\n",
    "total_terms = {}\n",
    "for doc in docs:\n",
    "    for term, postings in inverted_index.items():\n",
    "        for posting in postings:\n",
    "            if posting['doc'] == doc['docno']:\n",
    "                total_terms[doc['docno']] = total_terms.get(doc['docno'], 0) + posting['term_frequency']\n",
    "\n",
    "qlm_results = evaluate_qlm(queries, inverted_index, total_terms, smoothing_param=0.1)\n",
    "# Write results to a file\n",
    "write_results(qlm_results, 'QLM_JM_results.txt', 'QLM_JM_Run')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7f8fb",
   "metadata": {},
   "source": [
    "# EVALUATION - NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "04e9c143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Method: bm25\n",
      "NDCG@5: 0.3691850297076175\n",
      "NDCG@10: 0.37526805466757485\n",
      "Retrieval Method: vsm\n",
      "NDCG@5: 0.35308069516546847\n",
      "NDCG@10: 0.357273384482503\n",
      "Retrieval Method: QLM_JM\n",
      "NDCG@5: 0.053041299149673524\n",
      "NDCG@10: 0.060392399501209115\n"
     ]
    }
   ],
   "source": [
    "def read_relevance_file():\n",
    "    relevance_dict = {}\n",
    "    file_name = 'cranqrel.trec.txt'\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            relevance_score = int(parts[3])\n",
    "            if query_id not in relevance_dict:\n",
    "                relevance_dict[query_id] = {}\n",
    "            relevance_dict[query_id][doc_id] = relevance_score\n",
    "    return relevance_dict\n",
    "\n",
    "\n",
    "def read_results_file(retrieval_method):\n",
    "    results_dict = {}\n",
    "    directory = 'outputs'\n",
    "    file_name = f'{retrieval_method}_results.txt'\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            query_id = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            rank = int(parts[3])\n",
    "            if query_id not in results_dict:\n",
    "                results_dict[query_id] = []\n",
    "            results_dict[query_id].append(doc_id)\n",
    "    return results_dict\n",
    "\n",
    "\n",
    "def calculate_dcg(relevance_scores, k):\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevance_scores))):\n",
    "        rel = relevance_scores[i]\n",
    "        dcg += (2 ** rel - 1) / (i + 2)\n",
    "    return dcg\n",
    "\n",
    "def calculate_ndcg(relevance_judgments, retrieval_methods, k_values):\n",
    "    ndcg_scores = {}\n",
    "    for retrieval_method in retrieval_methods:\n",
    "        ndcg_scores[retrieval_method] = {}\n",
    "        for k in k_values:\n",
    "            ndcg_total = 0.0\n",
    "            query_count = 0\n",
    "            search_results = read_results_file(retrieval_method)\n",
    "            for query_id, relevant_docs in relevance_judgments.items():\n",
    "                if query_id in search_results:\n",
    "                    retrieved_docs = search_results[query_id]\n",
    "                    relevance_scores = [relevant_docs.get(doc_id, 0) for doc_id in retrieved_docs]\n",
    "                    idcg = calculate_dcg(sorted(relevance_scores, reverse=True), k)\n",
    "                    dcg = calculate_dcg(relevance_scores, k)\n",
    "                    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "                    ndcg_total += ndcg\n",
    "                    query_count += 1\n",
    "            ndcg_scores[retrieval_method][k] = ndcg_total / query_count if query_count > 0 else 0.0\n",
    "    return ndcg_scores\n",
    "\n",
    "\n",
    "# Read relevance judgments\n",
    "relevance_judgments = read_relevance_file()\n",
    "\n",
    "# Calculate NDCG@5 and NDCG@10 for BM25 and VSM\n",
    "retrieval_methods = ['bm25', 'vsm', 'QLM_JM']\n",
    "k_values = [5, 10]\n",
    "ndcg_scores = calculate_ndcg(relevance_judgments, retrieval_methods, k_values)\n",
    "\n",
    "# Print NDCG scores for each retrieval method and each value of k\n",
    "for method, scores in ndcg_scores.items():\n",
    "    print(f\"Retrieval Method: {method}\")\n",
    "    for k, ndcg in scores.items():\n",
    "        print(f\"NDCG@{k}: {ndcg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda329a",
   "metadata": {},
   "source": [
    "# APPENDIX - models not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8820b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regular Query Likelihood Model & Document Model (No Smoothing) ### \n",
    "def compute_query_likelihood(query, inverted_index, total_terms):\n",
    "    query_terms = preprocess(query['title'])\n",
    "    doc_likelihoods = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            postings = inverted_index[term]\n",
    "            for posting in postings:\n",
    "                doc_id = posting['doc']\n",
    "                term_freq = posting['term_frequency']\n",
    "                doc_length = total_terms.get(doc_id, 0)\n",
    "                likelihood = (term_freq / doc_length) if doc_length > 0 else 0\n",
    "                doc_likelihoods[doc_id] = doc_likelihoods.get(doc_id, 1) * likelihood\n",
    "    return doc_likelihoods\n",
    "\n",
    "def rank_qlm(query_likelihoods):\n",
    "    return sorted(query_likelihoods.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def evaluate_qlm(queries, inverted_index, total_terms):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        query_likelihoods = compute_query_likelihood(query, inverted_index, total_terms)\n",
    "        ranked_docs = rank_qlm(query_likelihoods)\n",
    "        for doc_id, likelihood in ranked_docs:\n",
    "            results.append((query['num'], doc_id, likelihood))\n",
    "    return results\n",
    "\n",
    "# Compute total number of terms per document (The Document Model)\n",
    "total_terms = {}\n",
    "for doc in docs:\n",
    "    for term, postings in inverted_index.items():\n",
    "        for posting in postings:\n",
    "            if posting['doc'] == doc['docno']:\n",
    "                total_terms[doc['docno']] = total_terms.get(doc['docno'], 0) + posting['term_frequency']\n",
    "# Evaluate QLM\n",
    "qlm_results = evaluate_qlm(queries, inverted_index, total_terms)\n",
    "\n",
    "# Write QLM results to a file\n",
    "write_results(qlm_results, 'QLM_results.txt', 'QLM_Run')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "64f3eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Trying out both Jelinek-Mercer and Dirichlet smoothing\n",
    "def compute_collection_freq(inverted_index):\n",
    "    collection_freq = {}\n",
    "    total_docs = len(inverted_index)\n",
    "    for term, postings in inverted_index.items():\n",
    "        collection_freq[term] = sum(posting['term_frequency'] for posting in postings) / total_docs\n",
    "    return collection_freq\n",
    "\n",
    "def compute_query_likelihood_jm(query, inverted_index, total_terms, collection_freq, smoothing_param):\n",
    "    query_terms = preprocess(query['title'])\n",
    "    doc_likelihoods = {}\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            postings = inverted_index[term]\n",
    "            for posting in postings:\n",
    "                doc_id = posting['doc']\n",
    "                term_freq = posting['term_frequency']\n",
    "                doc_length = total_terms.get(doc_id, 0)\n",
    "                likelihood = (1 - smoothing_param) * (term_freq / doc_length) + smoothing_param * collection_freq.get(term, 0)\n",
    "                doc_likelihoods[doc_id] = doc_likelihoods.get(doc_id, 1) * likelihood\n",
    "    return doc_likelihoods\n",
    "\n",
    "def compute_query_likelihood_dirichlet(query, inverted_index, total_terms, collection_freq, mu):\n",
    "    query_terms = preprocess(query['title'])\n",
    "    doc_likelihoods = {}\n",
    "    collection_size = sum(total_terms.values())\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            postings = inverted_index[term]\n",
    "            for posting in postings:\n",
    "                doc_id = posting['doc']\n",
    "                term_freq = posting['term_frequency']\n",
    "                doc_length = total_terms.get(doc_id, 0)\n",
    "                likelihood = (term_freq + mu * (collection_freq.get(term, 0) / collection_size)) / (doc_length + mu)\n",
    "                doc_likelihoods[doc_id] = doc_likelihoods.get(doc_id, 1) * likelihood\n",
    "    return doc_likelihoods\n",
    "\n",
    "# Define the evaluate_query_model function to handle both Jelinek-Mercer and Dirichlet smoothing\n",
    "def evaluate_query_model(queries, inverted_index, total_terms, collection_freq=None, smoothing_param=None, mu=None):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        if smoothing_param is not None:\n",
    "            query_likelihoods = compute_query_likelihood_jm(query, inverted_index, total_terms, collection_freq, smoothing_param)\n",
    "        elif mu is not None:\n",
    "            query_likelihoods = compute_query_likelihood_dirichlet(query, inverted_index, total_terms, collection_freq, mu)\n",
    "        ranked_docs = rank_qlm(query_likelihoods)\n",
    "        for doc_id, likelihood in ranked_docs:\n",
    "            results.append((query['num'], doc_id, likelihood))\n",
    "    return results\n",
    "\n",
    "# Compute collection frequency\n",
    "collection_freq = compute_collection_freq(inverted_index)\n",
    "\n",
    "# Evaluate and Output results for Jelinek-Mercer Smoothing\n",
    "smoothing_param = 0.1  # Set your chosen value for Jelinek-Mercer smoothing\n",
    "jm_results = evaluate_query_model(queries, inverted_index, total_terms, collection_freq=collection_freq, smoothing_param=smoothing_param)\n",
    "write_results(jm_results, 'QLM_JM_results.txt', f'QLM_JM_Run_{smoothing_param}')\n",
    "\n",
    "# Evaluate and Output results for Dirichlet Smoothing\n",
    "mu = 1000  # Set your chosen value for Dirichlet smoothing\n",
    "dirichlet_results = evaluate_query_model(queries, inverted_index, total_terms, collection_freq=collection_freq, mu=mu)\n",
    "write_results(dirichlet_results, 'QLM_D_results.txt', f'QOLM_D_Run_{mu}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f7d173da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KL Divergence ####\n",
    "\n",
    "# preprocess text\n",
    "def preprocess(text):\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "# compute KL divergence\n",
    "def kl_divergence(p, q):\n",
    "    return sum(p[i] * math.log2(p[i] / q[i]) for i in range(len(p)))\n",
    "\n",
    "# query likelihood with Jelinek-Mercer smoothing and KL divergence\n",
    "def compute_query_likelihood(query, inverted_index, total_terms, smoothing_param=0.1):\n",
    "    query_terms = preprocess(query['title'])\n",
    "    doc_likelihoods = {}\n",
    "    collection_frequencies = {term: sum(posting['term_frequency'] for posting in postings) for term, postings in inverted_index.items()}\n",
    "    total_collection_terms = sum(collection_frequencies.values())\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            postings = inverted_index[term]\n",
    "            for posting in postings:\n",
    "                doc_id = posting['doc']\n",
    "                term_freq = posting['term_frequency']\n",
    "                doc_length = total_terms.get(doc_id, 0)\n",
    "                term_prob = (1 - smoothing_param) * (term_freq / doc_length) + smoothing_param * (collection_frequencies.get(term, 0) / total_collection_terms)\n",
    "                doc_likelihoods[doc_id] = doc_likelihoods.get(doc_id, 1) * term_prob \n",
    "    # Normalize probabilities\n",
    "    total_prob = sum(doc_likelihoods.values())\n",
    "    for doc_id in doc_likelihoods:\n",
    "        doc_likelihoods[doc_id] /= total_prob   \n",
    "    return doc_likelihoods\n",
    "\n",
    "# rank documents based on query likelihood\n",
    "def rank_qlm(query_likelihoods):\n",
    "    return sorted(query_likelihoods.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# evaluate Query Likelihood Model with KL divergence\n",
    "def evaluate_qlm(queries, inverted_index, total_terms, smoothing_param=0.1):\n",
    "    results = []\n",
    "    for query in queries:\n",
    "        query_likelihoods = compute_query_likelihood(query, inverted_index, total_terms, smoothing_param)\n",
    "        ranked_docs = rank_qlm(query_likelihoods)\n",
    "        for doc_id, likelihood in ranked_docs:\n",
    "            results.append((query['num'], doc_id, likelihood))\n",
    "    return results\n",
    "\n",
    "# Compute total number of terms per document (document model)\n",
    "total_terms = {}\n",
    "for doc in docs:\n",
    "    for term, postings in inverted_index.items():\n",
    "        for posting in postings:\n",
    "            if posting['doc'] == doc['docno']:\n",
    "                total_terms[doc['docno']] = total_terms.get(doc['docno'], 0) + posting['term_frequency']\n",
    "\n",
    "# Evaluate QLM with Jelinek-Mercer smoothing\n",
    "qlm_results = evaluate_qlm(queries, inverted_index, total_terms, smoothing_param=0.1)\n",
    "\n",
    "# Write QLM results to a file\n",
    "write_results(qlm_results, 'QLM_KLD_results.txt', 'QLM_KLD_Run')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
